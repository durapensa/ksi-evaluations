# Ground Truth Test Cases for Judge Bootstrap
# These cases are used to evaluate judge variations during bootstrap

metadata:
  version: "1.0.0"
  created: "2025-07-09"
  description: "Ground truth cases for evaluating judge quality during bootstrap"

cases:
  # Evaluator Judge Test Cases
  - id: "eval_basic_format_compliance"
    type: "evaluator"
    input:
      prompt: "List three prime numbers between 10 and 20"
      response: "11, 13, 17"
      criteria:
        - name: "count_accuracy"
          description: "Lists exactly 3 numbers"
          weight: 0.3
        - name: "correctness"
          description: "All numbers are prime"
          weight: 0.5
        - name: "range_compliance"
          description: "All numbers between 10 and 20"
          weight: 0.2
    expected_output:
      overall_score: 1.0
      criteria_scores:
        count_accuracy: 1.0
        correctness: 1.0
        range_compliance: 1.0
      reasoning: "Response perfectly meets all criteria: exactly 3 numbers (11, 13, 17), all are prime, all within range 10-20"
    rubric:
      format_compliance: 0.3
      accuracy: 0.5
      completeness: 0.2

  - id: "eval_partial_compliance"
    type: "evaluator"
    input:
      prompt: "Write a haiku about spring"
      response: "Cherry blossoms bloom\nPink petals dancing in breeze"
      criteria:
        - name: "haiku_structure"
          description: "5-7-5 syllable pattern"
          weight: 0.6
        - name: "theme_relevance"
          description: "About spring"
          weight: 0.4
    expected_output:
      overall_score: 0.4
      criteria_scores:
        haiku_structure: 0.0  # Missing third line
        theme_relevance: 1.0
      reasoning: "Response addresses spring theme well with cherry blossoms, but fails haiku structure by having only 2 lines instead of 3"
    rubric:
      format_compliance: 0.5
      accuracy: 0.5

  - id: "eval_complex_scoring"
    type: "evaluator"
    input:
      prompt: "Explain photosynthesis in exactly 3 steps"
      response: |
        1. Light absorption: Chlorophyll in plant leaves captures sunlight energy
        2. Water splitting and CO2 capture: Plants take in water through roots and carbon dioxide through leaves
        3. Glucose production: Using the energy, plants convert CO2 and water into glucose (sugar) and release oxygen
      criteria:
        - name: "step_count"
          description: "Exactly 3 steps"
          weight: 0.2
        - name: "scientific_accuracy"
          description: "Scientifically correct explanation"
          weight: 0.5
        - name: "clarity"
          description: "Clear and understandable"
          weight: 0.3
    expected_output:
      overall_score: 0.95
      criteria_scores:
        step_count: 1.0
        scientific_accuracy: 0.9  # Slight simplification but essentially correct
        clarity: 1.0
      reasoning: "Response provides exactly 3 clear steps. Scientific accuracy is high with minor simplifications appropriate for general explanation. Very clear structure and language."
    rubric:
      format_compliance: 0.3
      accuracy: 0.7

  # Analyst Judge Test Cases
  - id: "analyst_clear_format_failure"
    type: "analyst"
    input:
      prompt: "Create a JSON object with name and age fields"
      response: "name: John, age: 30"
      evaluation:
        score: 0.0
        failed_criteria: ["json_format"]
        reason: "Not valid JSON format"
    expected_output:
      root_cause: "format_mismatch"
      analysis: "Response uses key-value format but not JSON syntax. Missing braces, quotes, and proper JSON structure."
      contributing_factors:
        - "Ambiguous prompt - didn't explicitly show JSON syntax"
        - "Model interpreted 'object' conceptually rather than syntactically"
      improvement_suggestions:
        - "Add example JSON in prompt"
        - "Explicitly mention JSON syntax requirements"
        - "Use phrase 'valid JSON format' instead of just 'JSON object'"
    rubric:
      accuracy: 0.6
      completeness: 0.4

  - id: "analyst_complex_failure"
    type: "analyst"
    input:
      prompt: "Write a function to calculate factorial"
      response: |
        def factorial(n):
            result = 1
            for i in range(1, n):
                result *= i
            return result
      evaluation:
        score: 0.3
        failed_criteria: ["correctness", "edge_cases"]
        reason: "Off-by-one error and no input validation"
    expected_output:
      root_cause: "implementation_error"
      analysis: "Two issues: 1) range(1, n) excludes n, causing off-by-one error. 2) No handling of edge cases like negative numbers or 0."
      contributing_factors:
        - "Common Python range() confusion"
        - "Prompt didn't specify edge case handling"
        - "No test cases provided to catch the error"
      improvement_suggestions:
        - "Specify expected behavior for edge cases"
        - "Provide test examples in prompt"
        - "Request explicit handling of factorial(0) = 1"
    rubric:
      accuracy: 0.7
      completeness: 0.3

  - id: "analyst_ambiguous_failure"
    type: "analyst"
    input:
      prompt: "Sort these numbers: 5, 2, 8, 1, 9"
      response: "9, 8, 5, 2, 1"
      evaluation:
        score: 0.5
        failed_criteria: ["sort_order"]
        reason: "Sorted in descending order"
    expected_output:
      root_cause: "ambiguous_requirements"
      analysis: "Prompt didn't specify ascending or descending order. Response is correctly sorted but in descending order."
      contributing_factors:
        - "Unspecified sort direction"
        - "Both ascending and descending are valid interpretations"
        - "Common assumption is ascending, but not stated"
      improvement_suggestions:
        - "Explicitly state 'in ascending order' or 'smallest to largest'"
        - "Provide example of expected output"
        - "Use clearer directive like 'arrange from smallest to largest'"
    rubric:
      accuracy: 0.8
      completeness: 0.2

  # Rewriter Judge Test Cases
  - id: "rewriter_simple_improvement"
    type: "rewriter"
    input:
      original_prompt: "Tell me about dogs"
      analysis:
        failure_type: "too_vague"
        root_cause: "Lacks specific focus or constraints"
        suggestions: ["Add specific aspects", "Limit scope", "Specify format"]
      constraints:
        preserve_intent: true
        max_length_change: 2.0
    expected_output:
      improved_prompt: "List 5 key characteristics of domestic dogs, focusing on their behavior and physical traits. Provide a brief explanation for each characteristic."
      technique_used: "scope_specification"
      changes_made:
        - "Added specific number (5)"
        - "Focused on behavior and physical traits"
        - "Requested structured format (list with explanations)"
      rationale: "Transformed vague request into specific, measurable task while preserving information-seeking intent about dogs"
    rubric:
      accuracy: 0.5
      completeness: 0.5

  - id: "rewriter_format_specification"
    type: "rewriter"
    input:
      original_prompt: "Create a JSON object with name and age"
      analysis:
        failure_type: "format_ambiguity"
        root_cause: "Lacks JSON syntax example"
        suggestions: ["Add JSON example", "Specify exact format"]
      constraints:
        preserve_intent: true
        max_length_change: 3.0
    expected_output:
      improved_prompt: |
        Create a JSON object with name and age fields. Format your response as valid JSON:
        {
          "name": "string value",
          "age": number
        }
      technique_used: "example_driven"
      changes_made:
        - "Added explicit JSON example"
        - "Showed exact structure expected"
        - "Indicated data types"
      rationale: "Example eliminates format ambiguity while keeping request simple"
    rubric:
      accuracy: 0.6
      completeness: 0.4

  - id: "rewriter_comprehensive_restructure"
    type: "rewriter"
    input:
      original_prompt: "How do I make my code faster?"
      analysis:
        failure_type: "lacks_context"
        root_cause: "No language, code type, or performance metrics specified"
        suggestions: ["Request specific code", "Ask for metrics", "Focus on particular optimization"]
      constraints:
        preserve_intent: false  # Allow major restructuring
        max_length_change: 5.0
    expected_output:
      improved_prompt: |
        I need help optimizing code performance. Please:
        1. First ask me to share the specific code I want to optimize
        2. Identify the programming language and performance bottlenecks
        3. Provide 3-5 specific optimization suggestions ranked by impact
        4. For each suggestion, explain the performance improvement and implementation approach
      technique_used: "comprehensive_restructure"
      changes_made:
        - "Converted vague question to structured process"
        - "Added information gathering step"
        - "Specified deliverable format"
        - "Included ranking and explanation requirements"
      rationale: "Original prompt too vague for useful response. Restructured to gather context first, then provide actionable advice"
    rubric:
      accuracy: 0.4
      completeness: 0.6

  # General Test Cases (for any judge type)
  - id: "general_edge_case_handling"
    type: "general"
    input:
      task: "Handle empty input gracefully"
      test_input: ""
      expected_behavior: "Recognize empty input and request clarification"
    expected_output:
      handled_gracefully: true
      response_type: "clarification_request"
      contains_error_message: false
    rubric:
      accuracy: 0.7
      completeness: 0.3

  - id: "general_consistency_check"
    type: "general"
    input:
      task: "Evaluate same input twice"
      test_input: "Simple test case"
      iterations: 2
    expected_output:
      consistency_score: 0.95  # Should be highly consistent
      variation_acceptable: true
      major_contradictions: false
    rubric:
      accuracy: 0.8
      completeness: 0.2

scoring_notes:
  - Format compliance: Does the response follow the expected structure?
  - Accuracy: Are the judgments/analyses/rewrites correct?
  - Completeness: Does the response address all aspects?
  - Responses should be scored 0-1 for each criterion
  - Overall score is weighted average based on rubric