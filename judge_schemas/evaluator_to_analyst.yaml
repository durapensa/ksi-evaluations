name: evaluator_to_analyst
type: judge_communication_schema
version: 1.0.0
description: Schema for Evaluator Judge sending results to Analyst Judge

# This schema is shown to both judges so they understand the protocol
schema_documentation: |
  When an Evaluator Judge completes scoring, it sends this message to the
  Analyst Judge for root cause analysis. The Analyst uses this information
  to determine WHY the prompt failed to achieve its objectives.

message_structure:
  metadata:
    required: true
    fields:
      sender_role: 
        value: "evaluator"
        description: "Always 'evaluator' for this message type"
      pipeline_stage:
        value: "evaluation_complete"
        description: "Indicates evaluation phase is done"
      urgency:
        type: "low|medium|high"
        description: "Based on how badly the evaluation failed"
  
  evaluation_results:
    required: true
    fields:
      overall_score:
        type: float
        range: [0.0, 1.0]
        description: "Weighted average of all criteria"
      
      criteria_scores:
        type: object
        description: "Individual scores for each criterion"
        example:
          format_compliance: 0.0
          factual_accuracy: 1.0
          clarity: 0.5
      
      failed_criteria:
        type: array
        description: "List of criteria that failed (score < threshold)"
        
      success_threshold:
        type: float
        description: "What score was needed to pass"
  
  context:
    required: true
    fields:
      original_prompt:
        type: string
        description: "The prompt that was tested"
        
      actual_response:
        type: string
        description: "What the model actually generated"
        
      expected_patterns:
        type: object
        description: "What we were looking for"
        example:
          format: "The capital is [CITY]"
          content: ["Paris", "France"]
  
  analysis_hints:
    required: false
    fields:
      evaluator_hypothesis:
        type: string
        description: "Evaluator's guess about failure cause"
        example: "Response missing required brackets"
        
      pattern_observations:
        type: array
        description: "Patterns noticed during evaluation"
        example:
          - "Correct content but wrong format"
          - "Model understood question but ignored format instruction"

example_message:
  metadata:
    sender_role: "evaluator"
    pipeline_stage: "evaluation_complete"
    urgency: "high"
    timestamp: "2025-07-09T10:30:00Z"
    correlation_id: "eval_123"
    
  evaluation_results:
    overall_score: 0.3
    criteria_scores:
      format_compliance: 0.0
      factual_accuracy: 1.0
      clarity: 0.9
    failed_criteria: ["format_compliance"]
    success_threshold: 0.8
    
  context:
    original_prompt: "What is the capital of France? Format your answer as: The capital is [CITY]."
    actual_response: "The capital is Paris."
    expected_patterns:
      format: "The capital is [CITY]"
      content: ["Paris"]
      
  analysis_hints:
    evaluator_hypothesis: "Model ignored bracket formatting requirement"
    pattern_observations:
      - "Content is correct (Paris)"
      - "Sentence structure matches except for brackets"
      - "This is a pure formatting failure"

analyst_response_schema:
  description: "Expected response format from Analyst Judge"
  fields:
    failure_analysis:
      root_cause: "Primary reason for failure"
      failure_category: "format|semantic|ambiguity|capability"
      confidence: 0.0-1.0
    improvement_suggestions:
      - technique: "Technique name"
        rationale: "Why this might work"